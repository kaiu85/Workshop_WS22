{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer-basierter Smalltalk-Chatbot\n",
    "### Kleiner Vorgeschmack auf Transformer\n",
    "\n",
    "Die sogenannte \"Transformer\"-Architektur stellt den State-of-the-Art in der Sprachverarbeitung dar. Wir werden uns im nächsten Workshop (8.5.) detailliert mit der Funktionsweise und dem Training dieser Klasse von neuronalen Netzen beschäftigen.\n",
    "\n",
    "Für heute wollen wir es jedoch dabei belassen, dass diese Netzwerke in der Lage sind, relativ komplexe und lange Texte zu verarbeiten und dabei auch Zusammenhänge zwischen weit voneinander entfernten Worten oder Textabschnitten im Auge zu behalten (mittels eines so genannten \"Attention\" (Aufmerksamkeits-) Mechanismus).\n",
    "\n",
    "Obwohl (oder vielleicht gerade da) die Transformer-Architektur eine der komplexeren Netzwerkstrukturen ist, gibt es eine sehr aktive und offene Community, die nicht nur entsprechende Publikationen, sondern auch die Netzwerkarchitekturen, Datensätze und sogar die fertig trainierte Netzwerke frei zur Verfügung stellt, z.B. über das Huggingface \"transformers\" Repository (https://github.com/huggingface/transformers). Im Folgenden werden wir aus diesem Repository (\"from transformers\") das \"pipeline\" Modul laden, welches für viele Aufgaben des Natural Language Processing schon vorgefertigte Pipelines (inclusive trainierter Transformernetzwerke) zur Verfügung stellt. Außerdem importieren wir die Klasse \"Conversation\", die es erlaubt, den Verlauf einer Conversation zu speichern und direkt an diese Transformernetzwerke zu übergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes erzeugen wir eine \"conversational\" Pipeline, die eine Chatbot-Konversation mit Hilfe eines votrainierten Transformernetzwerkes implementiert. Die vortrainierten Netzwerke, die zur Auswahl stehen, sind drei Versionen von DialogGPT, die sich im wesentlichen in ihrer Größe (d.h. der Anzahl der trainierten Netzwerkgewichte/Parameter) unterscheiden.\n",
    "Hier sind die drei \"Model cards\" dieser Modelle in dem Huggingface repository: \n",
    "\n",
    "- https://huggingface.co/microsoft/DialoGPT-small\n",
    "- https://huggingface.co/microsoft/DialoGPT-medium\n",
    "- https://huggingface.co/microsoft/DialoGPT-large\n",
    "\n",
    "In der nächsten Zelle erzeugen wir die entsprechende Pipeline, wobei der model = parameter bestimmt, welches Modell die Pipeline benutzen soll ('microsoft/DialoGPT-small', 'microsoft/DialoGPT-medium' oder 'microsoft/DialoGPT-large'). Beachten sie, dass alle diese Modelle auf einem großen Textkorpus von **englischen** Reddit-Konversationen trainiert wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_pipeline = pipeline(\"conversational\", model = 'microsoft/DialoGPT-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um eine Konversation mit Hilfe dieser Pipeline zu führen, benutzen wir eine \"Conversation\"-Struktur, die wir \"conversation1\" nennen. Diese behält den Überblick über die Benutzer-Inputs und die Bot-Antworten, d.h. über den gesamten Verlauf des Gespräches. Jede Konversation startet mit einem Nutzer-Input. Diesen speichern wir in der Variablen \"conversation_start\" und übergeben ihn \"Conversation\", um die entsprechende Struktur zu initialisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_start = \"Let's watch a movie tonight - any recommendations?\"\n",
    "\n",
    "# Erstelle eine \"Conversation\"-Struktur, die mit dem User-Input initialisiert wird,\n",
    "# der in \"conversation_start\" gespeichert ist\n",
    "conversation1 = Conversation(conversation_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"conversation1\" hilft uns nun dabei, den Überblick über den Verlauf unserer Unterhaltung zu behalten. Man kann jederzeit die gesamte bisherige Unterhaltung ausgeben mittels des folgenden Befehls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jede Instanz von \"Conversation\" (z.B. \"conversation1\") erhält eine eigene \"id\". Das erlaubt es der Conversational-Pipeline auch mehrere Conversations gleichzeitig zu bearbeiten. Wir bleiben aber zunächst bei einer einzelnen Konversation. Wir können nun unserer \"conversational_pipeline\" die entsprechende Konversation übergeben. Dann generiert das Transformer-Modell aus dem bisherigen Chatverlauf die nächste Antwort des Bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation1 = conversational_pipeline(conversation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der obige Befehl aktualisiert \"conversation1\", in dem er eine entsprechende Botantwort hinzufügt. Schauen wir uns den neuen Zustand unserer Konversation an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Konversation fortzuführen, fügen wir einen weitern User-Input hinzu. Das geht mittels conversaton1.add_user_input(...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_user_input = \"Did you like that movie?\"\n",
    "conversation1.add_user_input(next_user_input)\n",
    "\n",
    "print(conversation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir wieder conversational_pipeline nutzen, um die nächste Antwort zu generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation1 = conversational_pipeline(conversation1)\n",
    "\n",
    "print(conversation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen nicht immer auf die ganze Konversation zugreifen, sondern z.B. nur auf einzelne User-Inputs oder Chatbot-Outputs. Dies können wir über die entsprechenden Listen conversation1.past_user_inputs bzw. conversation1.generated_responses tun. Schauen wir uns zunächst die conversation1.past_user_inputs Liste an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation1.past_user_inputs) # Ganze Liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation1.past_user_inputs[0]) # Erster Eintrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation1.past_user_inputs[1]) # Zweiter Eintrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation1.past_user_inputs[-1]) # Letzter Eintrag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes schauen wir uns conversation1.generated_responses an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation1.generated_responses) # Ganze Liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation1.generated_responses[0]) # Erster Eintrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation1.generated_responses[1]) # Zweiter Eintrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation1.generated_responses[-1]) # Letzter Eintrag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kleines Projekt \n",
    "Können sie mit Hilfe der oben vorgeführten Transformers Conversational-Pipeline einen interaktiven Chatbot bauen, der sich an der Struktur unserer ersten Chatbot-Übung orientiert? Eine mögliche Lösung können sie durch Klick auf den Text ganz unten aufklappen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "\n",
    "print(\"Willkommen beim Chatbot\")\n",
    "print(\"Worüber würden Sie gerne heute sprechen?\")\n",
    "print(\"Zum beenden einfach 'bye' eintippen\")\n",
    "print(\"\")\n",
    "\n",
    "##### Hier kommt ihr Code (starten sie auch gerne mit ihrer Lösung der ersten\n",
    "##### Chatbot-Aufgabe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>**Klicken sie hier für eine mögliche Lösung**</summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "\n",
    "from transformers import pipeline, Conversation\n",
    "\n",
    "print(\"Wellcome to the Chat\")\n",
    "print(\"What would you like to talk about?\")\n",
    "print(\"Enter 'bye' to quit.\")\n",
    "print(\"\")\n",
    "\n",
    "conversational_pipeline = pipeline(\"conversational\")\n",
    "\n",
    "nutzereingabe = \"\"\n",
    "conv = Conversation()\n",
    "\n",
    "while nutzereingabe != \"bye\":\n",
    "    \n",
    "    nutzereingabe = input(\"Ihre Frage/Antwort: \")\n",
    "    \n",
    "    conv.add_user_input(nutzereingabe)\n",
    "    \n",
    "    res = conversational_pipeline(conv)\n",
    "    \n",
    "    print(res.generated_responses[-1])\n",
    "\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
